论文链接：[NeRF](https://dl.acm.org/doi/pdf/10.1145/3503250)

### NeRF 原理详解

其核心可以概括为：**用一个神经网络来“记住”一个三维场景中所有光线的样子。**

<img width="2200" height="616" alt="image" src="https://github.com/user-attachments/assets/8d3fa309-2527-484e-ba29-a921014d2abb" />

#### 第一步：将场景表示为一个连续的函数

传统的三维表示方法，如**网格（Mesh）**、**体素（Voxel）**或**点云（Point Cloud）**，都是离散的。它们用大量的三角形、小方块或点来近似描述一个物体的表面或体积。这种方式在存储和编辑上都有其局限性。

NeRF则另辟蹊径，它提出了一种**隐式（Implicit）**且**连续（Continuous）**的表示方法。它不直接存储场景的几何形状，而是训练一个简单的全连接神经网络（MLP，多层感知机），让这个网络本身成为场景的数学表达。

这个神经网络所代表的函数，我们称之为**神经辐射场**，其数学形式可以写作：

![F_{\Theta} : (x, y, z, \theta, \phi) \rightarrow (r, g, b, \sigma)](https://latex.codecogs.com/svg.latex?F_{\Theta}%20:%20(x,%20y,%20z,%20\theta,%20\phi)%20\rightarrow%20(r,%20g,%20b,%20\sigma))

我们来解读一下这个函数：

* **输入 (Input):**
    * **空间位置 (x, y, z):** 场景中任意一个点的三维坐标。
    * **视角方向 (θ, φ):** 观察这个点的二维方向（可以理解为水平角和垂直角）。
    这是一个 **5D向量**，因为它需要5个数字来确定。

* **输出 (Output):**
    * **颜色 (r, g, b):** 从指定的视角方向 ![(\theta, \phi)](https://latex.codecogs.com/svg.latex?(\theta,%20\phi)) 观察时，在 ![ (x, y, z) ](https://latex.codecogs.com/svg.latex?(x,%20y,%20z)) 这个点上所看到的颜色值。颜色与视角相关，是为了能模拟出镜面反射等效果（比如金属或水面的反光，你换个角度看，颜色会不一样）。
    * **体密度 (Volume Density, ![ \sigma ](https://latex.codecogs.com/svg.latex?\sigma)):** 这个值表示在 ![ (x, y, z) ](https://latex.codecogs.com/svg.latex?(x,%20y,%20z)) 这个点上存在“物质”的可能性有多大，或者说这个点有多“不透明”。如果 ![\sigma](https://latex.codecogs.com/svg.latex?\sigma) 值很高，说明这个点很可能是一个物体的表面；如果 ![\sigma](https://latex.codecogs.com/svg.latex?\sigma) 值为0，说明这个点是空的，光线可以直接穿过。体密度只与空间位置有关，与观察角度无关，因为一个物体在那儿，它的密度是客观存在的。

**核心思想：** 这个训练好的神经网络 ![F_{\Theta}](https://latex.codecogs.com/svg.latex?F_{\Theta})（![ \Theta ](https://latex.codecogs.com/svg.latex?\Theta) 代表网络的权重参数）就等同于整个三维场景。只要你给出空间中的任何一个点和观察方向，它就能告诉你那里的颜色和密度。它以一种连续的方式，编码了整个场景的几何形状和外观信息。

#### 第二步：利用体渲染（Volume Rendering）从函数中“画出”图像

现在我们有了一个能描述整个场景的函数，但如何将它变成我们能看到的二维图片呢？这里NeRF借鉴了计算机图形学中一个非常经典的技术——**体渲染**。

想象一下，当你的眼睛（或相机）看向一个场景时，你的视线会汇聚成一个**视锥（View Frustum）**。从相机出发，穿过屏幕上的某一个像素，会形成一条**光线（Ray）**。这条光线会穿过三维空间，直到射出场景边界。

体渲染要做的，就是计算这条光线在穿过场景时，沿途收集到的所有颜色，最终汇聚成这个像素点应该显示的颜色。

具体步骤如下：

1.  **光线步进（Ray Marching）：** 对于屏幕上的每一个像素，我们从相机位置沿着该像素的方向发射一条光线。然后，我们在这条光线上进行**采样**，即选取一系列离散的点，例如从近到远取N个点：![t_1, t_2, ..., t_N](https://latex.codecogs.com/svg.latex?t_1,%20t_2,%20...,%20t_N)。

2.  **查询网络：** 对于每一个采样点 ![p_i = (x_i, y_i, z_i)](https://latex.codecogs.com/svg.latex?p_i%20=%20(x_i,%20y_i,%20z_i))，我们将其坐标和光线的方向 ![d = (\theta, \phi)](https://latex.codecogs.com/svg.latex?d%20=%20(\theta,%20\phi)) 一起输入到我们训练好的神经网络 ![F_{\Theta}](https://latex.codecogs.com/svg.latex?F_{\Theta}) 中，得到该点的颜色 ![c_i = (r_i, g_i, b_i)](https://latex.codecogs.com/svg.latex?c_i%20=%20(r_i,%20g_i,%20b_i)) 和体密度 ![\sigma_i](https://latex.codecogs.com/svg.latex?\sigma_i)。

3.  **颜色积分（Color Integration）：** 现在我们有了光线上一系列点的颜色和密度，需要用一个**体积渲染方程**将它们“混合”成最终的像素颜色。这个方程的直观理解是：

    * 光线每前进一步，它遇到的颜色就会对最终的像素颜色产生贡献。
    * 这个贡献的大小，取决于该点的**颜色 ![\c_i](https://latex.codecogs.com/svg.latex?c_i)** 和**密度 ![\sigma_i](https://latex.codecogs.com/svg.latex?\sigma_i)**。密度越大，贡献越大。
    * 同时，光线在到达当前点之前，已经被前面的点**遮挡**了一部分。光线能“存活”到当前点的概率，我们称之为**透射率（Transmittance, ![T_i](https://latex.codecogs.com/svg.latex?T_i)）**。前面所有点的密度越高，![T_i](https://latex.codecogs.com/svg.latex?T_i) 就越低。

    最终像素的颜色 ![C(r)](https://latex.codecogs.com/svg.latex?C(r)) 是沿光线 ![r](https://latex.codecogs.com/svg.latex?r) 的所有采样点颜色的加权和：

    ![C(r) = \sum_{i=1}^{N} T_i \cdot (1 - \exp(-\sigma_i \cdot \delta_i)) \cdot c_i](https://latex.codecogs.com/svg.latex?C(r)%20=%20\sum_{i=1}^{N}%20T_i%20\cdot%20(1%20-%20\exp(-\sigma_i%20\cdot%20\delta_i)))

    其中：
    * ![\delta_i](https://latex.codecogs.com/svg.latex?\delta_i) 是相邻采样点之间的距离。
    * ![(1 - \exp(-\sigma_i \cdot \delta_i))](https://latex.codecogs.com/svg.latex?1%20-%20\exp(-\sigma_i%20\cdot%20\delta_i)) 表示光线在这一个小段距离内被吸收的概率。
    * ![T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j \cdot \delta_j)](https://latex.codecogs.com/svg.latex?T_i%20=%20\exp(-\sum_{j=1}^{i-1}%20\sigma_j%20\cdot%20\delta_j))，表示光线能够穿透到第 i 个点而不被遮挡的概率。

通过为屏幕上的每个像素都执行一次这个过程，我们就能渲染（“画”）出一张全新的、在任意视角下的二维图像。

#### 第三步：通过“对比”来训练网络

以上两步描述了一个训练好的NeRF是如何工作的。那么，这个网络最初是如何被训练的呢？

训练过程的目标，是让神经网络生成的图像与真实拍摄的图像尽可能地一致。

1.  **输入数据：** 我们需要一个包含多张从**已知相机位姿**（位置和朝向）拍摄的同一场景的图像集。

2.  **损失函数（Loss Function）：** 训练的核心是定义一个“惩罚”标准。在每一次训练迭代中：
    * 我们随机选取一张训练图像及其对应的相机位姿。
    * 对于该图像中的一批像素，我们利用当前的神经网络和体渲染方法，计算出这些像素的渲染颜色。
    * 我们计算**渲染颜色**与**真实图像中对应像素的颜色**之间的**均方误差（Mean Squared Error）**。这个误差值就是**损失（Loss）**。损失越大，说明网络生成的图像与真实图像差距越大。

3.  **优化（Optimization）：** 我们的目标是让这个损失尽可能小。由于整个过程（从输入坐标到最终像素颜色）都是**可微分（Differentiable）**的，我们可以使用**梯度下降（Gradient Descent）**等优化算法。通过计算损失相对于神经网络权重 ![\Theta](https://latex.codecogs.com/svg.latex?\Theta) 的梯度，我们可以知道应该如何调整这些权重，才能让下一次渲染出的图像更接近真实图像。

这个过程会重复成千上万次，每一次迭代，神经网络都会进行微小的调整，最终，它就“学会”了整个三维场景的辐射场。

#### 两个关键的优化技巧

为了让NeRF能够学习到精细的细节并提高效率，原始论文中还引入了两个重要的技巧：

1.  **位置编码（Positional Encoding）：** 直接将 ![(x, y, z)](https://latex.codecogs.com/svg.latex?(x,%20y,%20z)) 坐标输入神经网络，网络很难学习到场景中的高频细节（比如精细的纹理）。因此，NeRF在将坐标输入网络前，会先通过一系列不同频率的正弦和余弦函数（sin, cos）将其映射到一个更高维度的空间。这极大地帮助了网络捕捉和重建微小的细节。

2.  **分层体积采样（Hierarchical Volume Sampling）：** 如果在光线上均匀采样，很多采样点会落在空无一物的空间或是被完全遮挡的物体内部，这些点对最终颜色的贡献很小，造成了计算浪费。为了解决这个问题，NeRF采用了“由粗到精”的策略：
    * **粗网络（Coarse Network）：** 首先训练一个网络，在光线上进行稀疏的、均匀的采样，得到一个初步的颜色分布。
    * **精网络（Fine Network）：** 根据粗网络预测出的密度分布，我们知道光线上的哪些区域更可能是物体表面（即密度高的区域）。然后，我们在这些“重要”区域进行更密集的采样。
    * 将所有采样点（粗采样+精细采样）的结果输入到最终的**精网络**中进行渲染。这样既保证了渲染质量，又提高了计算效率。

