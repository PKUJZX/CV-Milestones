### cldm.py

`cldm.py` 是 ControlNet 项目的**灵魂**。它定义了 ControlNet 的核心架构和工作流，将一个可训练的控制模块（`ControlNet`）无缝地集成到一个预训练的、固定的隐空间扩散模型（如 Stable Diffusion）中。

我们将从下往上，按照 **控制模块 -> 被控制的U-Net -> 完整的扩散模型** 的顺序来讲解其中的三个关键类。

#### 1. `ControlNet` 类：控制信号的生成器

这个类是 ControlNet 的核心创新所在。它本身就是一个神经网络，专门用于从外部条件（`hint`）中提取空间信息，并将其转化为一系列可以指导主模型生成过程的“控制信号”。

**核心思想**：`ControlNet` 的网络结构是 Stable Diffusion `UNetModel` **编码器部分的一个完整、可训练的副本**。它“借用”了 U-Net 强大的特征提取能力。

**详细解析**：

- **初始化 (`__init__`)**:
  - 它接收与原始 U-Net 相同的参数（如 `image_size`, `in_channels`, `model_channels` 等），确保了结构上的兼容性。
  - **`input_hint_block`**: 这是一个小型的卷积网络。它的作用是**预处理 `hint`**（例如 Canny 边缘图）。它将 `hint` 从一个普通的图像（如 3 通道）转换成一个特征图，其维度与 U-Net 初始输入块的输出维度相匹配，为后续的注入做准备。
  - **编码器副本**: `self.input_blocks` 和 `self.middle_block` 的结构与 U-Net 的编码器和中间块完全相同。这是为了复用 Stable Diffusion 在大规模数据上学到的强大的视觉特征提取能力。
  - **`zero_convs` (零卷积)**: 这是 ControlNet 能够成功训练的**关键技巧之一**。在 `ControlNet` 的输出端，有一系列 1x1 的卷积层，它们的权重和偏置在初始化时**全部被设置为零**。
    - **目的**: 确保在训练刚开始时，`ControlNet` 的输出全部为零。这意味着它不会对原始的、已经训练得很好的 Stable Diffusion 模型产生任何影响。模型从一个“无害”的状态开始学习，避免了在训练初期因随机初始化的控制信号而破坏生成过程，从而保证了训练的稳定性。这被称为“无害初始化”。
  - **`control_scales`**: 这是一个用于控制每个控制信号强度的参数，在 `guess_mode` 下会动态调整。
- **前向传播 (`forward`)**:
  - **输入**:
    1. `x`: 来自主 U-Net 的带噪声的隐空间张量。
    2. `hint`: 外部的控制条件，例如一张 Canny 边缘图。
    3. `timesteps`: 当前的扩散时间步。
    4. `context`: 来自文本编码器的条件向量（即 prompt）。
  - **过程**:
    1. `hint` 首先通过 `input_hint_block` 进行编码。
    2. `x` 和 `hint` 在通道维度上拼接 (`torch.cat([x, hint], dim=1)`)，然后一起送入 `ControlNet` 的编码器部分。
    3. 数据流经 `input_blocks` 和 `middle_block`，就像在标准的 U-Net 编码器中一样。在每一层，`timesteps` 的嵌入和 `context` 的文本条件都会被融入计算。
  - **输出**:
    - 它**不输出**最终图像或预测的噪声。
    - 它的输出是一个列表 `outs`，包含了从 `ControlNet` 的 12 个不同尺度（来自 `input_blocks`）和中间块提取出的特征图。这些特征图经过了 `zero_convs` 处理，它们就是最终要施加给主 U-Net 的“控制信号”。

#### 2. `ControlledUnetModel` 类：被注入控制的 U-Net

这个类是对 Stable Diffusion 原始 `UNetModel` 的一个封装或修改。它的唯一目的就是在 U-Net 的解码（上采样）阶段，将 `ControlNet` 生成的控制信号“注入”进去。

**核心思想**：不修改 U-Net 内部的复杂结构，只在解码器的跳跃连接（skip connection）处做手脚，将控制信号加到上面。

**详细解析**:

- **初始化 (`__init__`)**: 它接收一个已经实例化的 `ControlNet` 对象作为 `control_model`。

- **前向传播 (`forward`)**:

  - **输入**: 除了标准 U-Net 的输入外，它还额外接收一个 `control` 参数，这个 `control` 就是 `ControlNet` 在上一步生成的特征图列表 (`outs`)。

  - **过程**:

    1. **编码阶段**: 和原始 U-Net 完全一样。数据流经 `input_blocks` 和 `middle_block`，同时将每一层的输出（即跳跃连接的来源）保存在一个列表 `hs` 中。

    2. **解码阶段 (核心改动)**: 这是魔法发生的地方。在每个上采样块（`self.output_blocks`）中，当需要从 `hs` 中取出对应的跳跃连接特征图时，它会执行以下操作：

       ```python
       h = torch.cat([h, hs.pop() + control.pop()], dim=1)
       ```

       - `hs.pop()`: 从编码器获取原始的跳跃连接特征图。
       - `control.pop()`: 从 `ControlNet` 的输出中获取对应层级的控制信号。
       - `+`: 将两者**逐元素相加**。这就是控制的注入点！`ControlNet` 的空间信息被直接添加到了 U-Net 自身的特征之上。
       - `torch.cat(...)`: 将融合了控制信号的跳跃连接与上采样后的特征图拼接，然后继续后续的解码过程。

  - **输出**: 和原始 U-Net 一样，最终输出预测的噪声。但这个噪声现在受到了 `control` 信号的引导。

#### 3. `ControlLDM` 类：完整的可控扩散模型

这个类是最高层级的封装，它将 `ControlNet` 和 `ControlledUnetModel` 组合成一个完整的、可用于训练和推理的隐空间扩散模型（Latent Diffusion Model）。

**核心思想**: 继承自 LDM 的 `LatentDiffusion` 类，重写关键的训练和推理逻辑，将 ControlNet 的工作流整合进去。

**详细解析**:

- **初始化 (`__init__`)**:
  - 它接收一个 `control_stage_config`，这是一个字典，定义了 `ControlNet` 的具体模型结构。
  - **`instantiate_controlnet`**: 这个方法会根据配置创建 `ControlNet` 的实例，并将其赋值给 `self.control_model`。
  - 它将原始 U-Net (`self.model.diffusion_model`) 替换为我们上面定义的 `ControlledUnetModel` 的实例，并将 `self.control_model` 传递给它。
- **`apply_model` (核心方法)**: 这是在每个去噪步骤中被调用的核心函数。
  - **过程**:
    1. 它首先调用 `self.control_model`，传入 `x_noisy`, `t`, `cond` (文本条件) 和 `control` (hint) 来生成控制信号列表。
    2. 然后，它调用 `self.model.diffusion_model` (即 `ControlledUnetModel`)，传入 `x_noisy`, `t`, `cond` 以及刚刚从 `control_model` 得到的控制信号列表。
    3. 最后返回 `ControlledUnetModel` 预测出的噪声。
  - 这个方法清晰地展示了 **ControlNet 先行，U-Net 跟随** 的两步工作流。
- **`configure_optimizers` (训练策略的关键)**: 这个方法揭示了 ControlNet 的训练效率为何如此之高。
  - **`sd_locked` 参数**: 默认为 `True`。
  - **当 `sd_locked` 为 `True` 时**:
    - 它会遍历模型的所有参数，**只收集 `self.control_model` 的参数**作为可训练参数列表。
    - 原始 Stable Diffusion 模型（包括文本编码器、VAE 和 U-Net 的主体部分）的参数**全部被冻结** (`requires_grad = False`)。
  - **意义**: 这意味着训练时，我们不需要触动巨大的、已经预训练好的 Stable Diffusion 模型。我们只训练一个相对轻量级的 `ControlNet` 副本。这极大地降低了计算资源需求和训练时间，使得在消费级硬件上进行微调成为可能。
- **`low_vram_shift`**: 一个非常实用的功能。当显存不足时，它可以在不需要使用的模型部分（如 VAE 和 ControlNet）之间进行切换，将暂时不用的模型移动到 CPU 以释放宝贵的 GPU 显存，从而实现低显存推理。



### infer.py

#### 1. 准备工作：加载模型和采样器

- **创建模型结构**:

  ```python
  model = create_model('./models/cldm_v15.yaml').cpu()
  ```

  这一行代码读取 `cldm_v15.yaml` 配置文件。这个 YAML 文件定义了整个 `ControlLDM` 模型的完整架构，包括 ControlNet 部分和被控制的 U-Net 部分。`create_model` 函数会根据这个“蓝图”在内存中构建出模型的骨架。

- **加载模型权重**:

  ```python
  model.load_state_dict(load_state_dict('./models/v1-5-pruned-emaonly.safetensors', location='cpu'), strict=False)
  model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cpu'), strict=False)
  ```

  这里分两步加载权重：

  1. 首先，加载基础的 **Stable Diffusion v1.5 模型** (`v1-5-pruned-emaonly.safetensors`)。这是模型的“底座”，提供了强大的文生图能力。
  2. 然后，加载**训练好的 ControlNet 权重** (`control_sd15_canny.pth`)。这些权重包含了从 Canny 边缘图提取空间信息并指导生成过程的“知识”。

  - **`strict=False`** 参数至关重要。因为 `control_sd15_canny.pth` 文件只包含 `ControlNet` 模块的参数，而不包含整个 Stable Diffusion 模型的参数。设置 `strict=False` 允许我们只加载文件中存在的权重，而忽略那些不存在的（比如 Stable Diffusion 主干网络的权重）。

- **创建采样器**:

  ```python
  ddim_sampler = DDIMSampler(model)
  ```

  这里实例化了一个 `DDIMSampler`。采样器是扩散模型的执行者，它负责在指定的步数内，按照模型的指导，一步步地从纯噪声中“雕刻”出清晰的图像。DDIM 是一种高效的采样算法。

#### 2. 准备输入：文本提示和控制信号

模型加载完毕后，我们需要为它准备两种输入：

- **文本提示 (Prompt)**:

  ```
  prompt = "A beautiful, detailed, and realistic image of a running horse"
  a_prompt = 'best quality, extremely detailed'
  n_prompt = 'longbody, lowres, bad anatomy, ...'
  ```

  - `prompt`: 你想要生成的核心内容。
  - `a_prompt` (additional prompt): 正向提示，用于提升画面质量和细节。
  - `n_prompt` (negative prompt): 负向提示，告诉模型要避免生成哪些内容，可以有效防止一些常见的生成缺陷。

- **控制信号 (Hint)**: 这是 ControlNet 的精髓所在。

  ```python
  input_image = Image.open('./test_imgs/dog.png')
  input_image = np.array(input_image)
  
  detected_map = cv2.Canny(input_image, low_threshold, high_threshold)
  detected_map = cv2.cvtColor(detected_map, cv2.COLOR_GRAY2BGR)
  ```

  1. 脚本首先加载一张**输入图像**（例如 `horse.png`）。
  2. 然后使用 OpenCV (`cv2`) 的 `Canny` 函数对这张图像进行**边缘检测**。
  3. 生成的 `detected_map` (一张黑底白边的图像) 就是提供给 `ControlNet` 的**控制信号（`hint`）**。这张图精确地定义了生成图像中狗的轮廓和姿态。

#### 3. 执行核心：采样与生成

一切准备就绪，现在开始生成图像：

- **构建条件 (`cond`) 和无条件 (`un_cond`) 输入**:

  ```python
  control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0
  # ...
  cond = {"c_concat": [control], "c_crossattn": [model.get_learned_conditioning([prompt + ', ' + a_prompt])]}
  un_cond = {"c_concat": [control], "c_crossattn": [model.get_learned_conditioning([n_prompt])]}
  ```

  这是最关键的一步。脚本准备了两个字典，它们将作为输入传递给采样器：

  - **`cond` (条件输入)**:
    - `c_concat`: 包含了我们的**控制信号** `control` (Canny 边缘图)。
    - `c_crossattn`: 包含了编码后的**正向文本提示**。
  - **`un_cond` (无条件输入)**:
    - `c_concat`: 通常也包含 `control`，以确保引导和非引导过程的结构一致性。
    - `c_crossattn`: 包含了编码后的**负向文本提示**。

  这两个字典的组合用于**无分类器指导 (Classifier-Free Guidance, CFG)**，这是一种能显著提升生成图像质量和与提示词匹配度的技术。

- **运行采样器**:

  ```python
  samples, intermediates = ddim_sampler.sample(
      S=ddim_steps,
      conditioning=cond,
      # ... 其他参数 ...
      unconditional_guidance_scale=scale,
      unconditional_conditioning=un_cond,
  )
  ```

  调用 `ddim_sampler.sample` 方法启动生成过程。在 `ddim_steps`（例如 20）步的迭代中，采样器会在每一步调用 `ControlLDM` 的 `apply_model` 方法来预测噪声。`apply_model` 内部会先运行 `ControlNet` 处理 Canny 边缘图，然后将生成的控制信号注入到主 U-Net 中，从而确保最终生成的图像严格遵循 Canny 图的轮廓。

#### 4.收尾工作：解码和保存

采样完成后，我们得到的是在隐空间（latent space）中的特征图 `samples`。

- **解码图像**:

  ```python
  x_samples = model.decode_first_stage(samples)
  ```

  调用模型的 `decode_first_stage` 方法（即 VAE 解码器），将隐空间特征图转换回我们肉眼可见的像素图像。

- **后处理和保存**:

  ```python
  x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)
  # ...
  Image.fromarray(np.concatenate(results, axis=1)).save('./test_imgs/horse_out.png')
  ```

  脚本将解码后的图像从 Tensor 格式转换为 Numpy 数组，并将像素值从 [-1, 1] 的范围恢复到 [0, 255]。最后，会将**原始输入图、Canny 边缘图和最终生成的图像**拼接在一起保存，方便直观地对比效果。