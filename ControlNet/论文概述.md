原文链接：[Adding Conditional Control to Text-to-Image Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)

### ControlNet

#### 1. 基本设定：神经网络块 (Neural Network Block)
在复杂的神经网络中，计算通常被组织成一个个标准化的“块”（Block），例如ResNet块或Transformer块。这些块的功能可以用一个函数来表示：

<img src="https://latex.codecogs.com/svg.latex?y&space;=&space;\mathcal{F}(x;&space;\Theta)" alt="y = F(x; Theta)" />

* <img src="https://latex.codecogs.com/svg.latex?x" alt="x" /> 是输入的特征图（feature map）。
* <img src="https://latex.codecogs.com/svg.latex?\mathcal{F}" alt="F" /> 代表这个“块”所执行的一系列运算。
* <img src="https://latex.codecogs.com/svg.latex?\Theta" alt="Theta" /> 代表这个“块”的所有参数（即网络权重和偏置）。
* <img src="https://latex.codecogs.com/svg.latex?y" alt="y" /> 是经过这个“块”处理后输出的结果。

#### 2. 核心架构：锁定与复制 (Lock and Copy)
这是ControlNet最关键的设计。当需要将控制能力添加到一个预训练好的网络块上时，它会执行两个关键操作：
* **锁定 (Lock/Freeze)**：将原始网络块的参数 <img src="https://latex.codecogs.com/svg.latex?\Theta" alt="Theta" /> 完全冻结，使其在后续的训练中绝对不会发生任何改变。这一步的目的是**完整保留**大型预训练模型已经学到的宝贵知识和强大的图像生成能力。
* **复制 (Clone)**：完整地复制一份这个被锁定的块，得到一个“**可训练副本**”（Trainable Copy）。这个副本的参数记为 <img src="https://latex.codecogs.com/svg.latex?\Theta_c" alt="Theta_c" />，与原始块不同，它的参数是可以在训练中被更新和修改的。这个副本的核心任务就是去学习如何理解并整合我们给出的外部控制条件。

#### 3. 控制的来源：外部条件 (External Condition)
这个新创建的“可训练副本”会接收一个额外的输入数据 <img src="https://latex.codecogs.com/svg.latex?c" alt="c" />，这 <img src="https://latex.codecogs.com/svg.latex?c" alt="c" /> 就是我们用来施加控制的**控制信号**。这个信号可以是多种形式的，例如人体姿态的骨骼图、图像的轮廓线稿或场景的深度图。

#### 4. 巧妙的连接器：零卷积层 (Zero Convolution Layers)
如何将“可训练副本”学到的控制信息，安全无害地传递给“锁定的原始模型”呢？ControlNet为此设计了**零卷积层**，记为 <img src="https://latex.codecogs.com/svg.latex?\mathcal{Z}(.;.)" alt="Z(.;.)" />。
* **定义**：它是一种特殊的1x1卷积层，其**权重和偏置在初始化时都被强制设为0**。
* **作用**：
    1.  **保证训练稳定启动**：在训练刚开始时，由于零卷积层的权重和偏置都是0，所以无论其输入是什么，它的输出也必定是0。这可以有效防止副本在参数尚未训练好时产生的随机“噪声”干扰和破坏原始模型的稳定输出。
    2.  **实现渐进式学习**：随着训练的进行，零卷积层的参数会从0开始，逐渐学习到非零的权重。这使得控制信号能够被平滑、渐进地添加到原始模型的特征中，整个学习过程更加稳定和高效。

#### 5. 完整的计算流程
一个应用了ControlNet的块，其最终输出 <img src="https://latex.codecogs.com/svg.latex?y_c" alt="y_c" /> 由以下公式定义：

<img src="https://latex.codecogs.com/svg.latex?y_c&space;=&space;\mathcal{F}(x;&space;\Theta)&space;&plus;&space;\mathcal{Z}(\mathcal{F}(x&space;&plus;&space;\mathcal{Z}(c;&space;\Theta_{z1});&space;\Theta_c);&space;\Theta_{z2})" alt="y_c = F(x;Theta) + Z(F(x+Z(c;Theta_z1);Theta_c);Theta_z2))" />

我们来分解这个公式：
* <img src="https://latex.codecogs.com/svg.latex?\mathcal{F}(x;&space;\Theta)" alt="F(x; Theta)" />：这是**原始锁定块**的输出，保持原样。
* <img src="https://latex.codecogs.com/svg.latex?\mathcal{Z}(c;&space;\Theta_{z1})" alt="Z(c; Theta_z1)" />：第一个零卷积层处理输入的**控制条件 <img src="https://latex.codecogs.com/svg.latex?c" alt="c" />**。
* <img src="https://latex.codecogs.com/svg.latex?x&space;&plus;&space;\mathcal{Z}(c;&space;\Theta_{z1})" alt="x + Z(c; Theta_z1)" />：将处理后的控制条件与原始输入 <img src="https://latex.codecogs.com/svg.latex?x" alt="x" /> 相加。
* <img src="https://latex.codecogs.com/svg.latex?\mathcal{F}(&space;...&space;;&space;\Theta_c)" alt="F( ... ; Theta_c)" />：将上述混合了控制条件的输入，送入**可训练副本**中进行计算。
* <img src="https://latex.codecogs.com/svg.latex?\mathcal{Z}(&space;...&space;;&space;\Theta_{z2})" alt="Z( ... ; Theta_z2)" />：将可训练副本的输出，再通过第二个零卷积层进行处理。
* <img src="https://latex.codecogs.com/svg.latex?&plus;" alt="+" />：最后，将**原始块的输出**与**处理后的副本输出**直接相加，得到最终结果 <img src="https://latex.codecogs.com/svg.latex?y_c" alt="y_c" />。

由于零卷积层的特性，在训练开始的第一步，公式的第二项为零，因此：

<img src="https://latex.codecogs.com/svg.latex?y_c&space;=&space;y" alt="y_c = y" />

这意味着ControlNet在开始时对原模型毫无影响，这是一个非常安全和聪明的训练起点。



### 文生图应用
#### 1. 应用基础：Stable Diffusion的架构

要理解ControlNet如何工作，首先要了解它所应用的平台——Stable Diffusion。

* **核心结构是U-Net**：Stable Diffusion本质上是一个U-Net结构，包含三个主要部分：一个编码器（Encoder）、一个中间块（Middle Block）和一个带有“跳跃连接”（Skip-connections）的解码器（Decoder）。
* **工作于潜空间 (Latent Space)**：为了提高效率，Stable Diffusion并不直接处理高分辨率的像素图像（如512x512），而是先用一个预处理模型（如VQ-GAN）将图像压缩成一个更小的“潜空间”表征（如64x64）。所有的核心扩散过程都在这个潜空间中进行。
* **包含多个模块**：其编码器和解码器分别由12个模块构成，加上中间块，总共有25个核心模块。这些模块内部由ResNet层和ViTs等复杂结构组成，负责处理图像特征和文本提示（通过CLIP编码器）的融合。

#### 2. 控制信号的预处理

由于Stable Diffusion在64x64的潜空间工作，我们提供的控制条件（如一张512x512的边缘图或姿态图）也必须被转换成匹配的格式。

* **专用编码器 <img src="https://latex.codecogs.com/svg.latex?\mathcal{E}(.)" alt="E(.)" />**：ControlNet为此设计了一个小型的、独立的神经网络编码器 <img src="https://latex.codecogs.com/svg.latex?\mathcal{E}(.)" alt="E(.)" />。这个网络由4个卷积层构成。
* **转换过程**：这个小型编码器 <img src="https://latex.codecogs.com/svg.latex?\mathcal{E}(.)" alt="E(.)" /> 的任务就是将高分辨率的输入控制图像 <img src="https://latex.codecogs.com/svg.latex?c_i" alt="c_i" /> （image-space condition）转换成一个低维度的特征空间条件向量 <img src="https://latex.codecogs.com/svg.latex?c_f" alt="c_f" /> (feature-space conditioning vector)。

<img src="https://latex.codecogs.com/svg.latex?c_f&space;=&space;\mathcal{E}(c_i)" alt="c_f = E(c_i)" />

* **联合训练**：这个小型编码器 <img src="https://latex.codecogs.com/svg.latex?\mathcal{E}(.)" alt="E(.)" /> 的参数是与ControlNet的主体结构一起进行训练的。最终生成的64x64的特征向量 <img src="https://latex.codecogs.com/svg.latex?c_f" alt="c_f" /> 才会作为控制信号，被送入到接下来的ControlNet结构中。

#### 3. ControlNet与U-Net的整合方式

ControlNet并不是简单地作用于整个Stable Diffusion，而是精准地嵌入其U-Net的编码器部分。

* **复制编码器和中间块**：ControlNet创建了Stable Diffusion中**全部12个编码器（Encoder）模块**和**1个中间块（Middle Block）**的可训练副本。
* **注入到跳跃连接**：这些副本的输出，并不会干扰原始U-Net的内部数据流。相反，它们的输出被逐一**添加（add）**到U-Net解码器所使用的**12个跳跃连接（skip-connections）**上，以及添加到**中间块**的输出上。

这种注入方式非常巧妙，因为它没有改变U-Net原有的“编码-解码”结构，而是像在U-Net的“桥梁”（跳跃连接）上增加了额外的引导信息，从而在不破坏结构的前提下实现了控制。

#### 4. 实现的效率与优势

这种设计带来了巨大的计算效率优势。

* **无需计算原始梯度**：在训练（微调）过程中，由于Stable Diffusion原始的编码器部分是被完全**锁定和冻结**的，因此完全不需要为它计算梯度。梯度计算只在可训练的ControlNet副本上进行。
* **节省训练成本**：这一特性极大地**加快了训练速度**并**节省了GPU显存**。
* **具体数据**：论文中提到，在单张NVIDIA A100 40GB GPU上进行测试，与从头优化Stable Diffusion相比，使用ControlNet进行优化：
    * 仅需要**增加约23%的GPU显存**。
    * 每个训练迭代仅**增加约34%的时间**。

这证明了ControlNet是一种在计算上非常高效的，为大型预训练模型增加控制能力的方法。



### 训练

#### 训练目标

ControlNet的训练目标与基础的扩散模型完全一致。模型的目标是预测在给定时间步 <img src="https://latex.codecogs.com/svg.latex?t" alt="t" /> 被添加到图像上的噪声 <img src="https://latex.codecogs.com/svg.latex?\epsilon" alt="epsilon" />。其损失函数 <img src="https://latex.codecogs.com/svg.latex?\mathcal{L}" alt="L" /> 如下：

<img src="https://latex.codecogs.com/svg.latex?\mathcal{L}&space;=&space;\mathbb{E}_{z_0,t,c_t,c_f,\epsilon\sim\mathcal{N}(0,1)}[||\epsilon&space;-&space;\epsilon_\theta(z_t,&space;t,&space;c_t,&space;c_f))||_2^2]" alt="L = E[...] ||epsilon - epsilon_theta(...) ||^2" />

与标准扩散模型相比，这里的关键区别在于模型的预测 <img src="https://latex.codecogs.com/svg.latex?\epsilon_\theta" alt="epsilon_theta" /> 不仅依赖于噪声图像 <img src="https://latex.codecogs.com/svg.latex?z_t" alt="z_t" />、时间步 <img src="https://latex.codecogs.com/svg.latex?t" alt="t" /> 和文本提示 <img src="https://latex.codecogs.com/svg.latex?c_t" alt="c_t" />，还依赖于一个新增的任务相关条件 <img src="https://latex.codecogs.com/svg.latex?c_f" alt="c_f" />（即来自ControlNet的控制信号，如姿态、深度图等）。

#### 训练技巧：随机空提示

为了增强ControlNet从图像条件（如边缘、姿态）中直接识别语义的能力，论文采用了一个重要的训练技巧：在训练过程中，有**50%的概率将文本提示 <img src="https://latex.codecogs.com/svg.latex?c_t" alt="c_t" /> 随机替换为空字符串**。

这个做法强迫模型不能仅仅依赖文本描述，而必须学会从输入的控制图像 <img src="https://latex.codecogs.com/svg.latex?c_f" alt="c_f" /> 中理解用户的意图。

#### “突然收敛”现象

一个有趣的观察是，ControlNet的学习过程并非平滑渐进的。由于零卷积的存在，模型从一开始就能生成高质量的图像。它并不会慢慢学会遵循控制条件，而是在训练的某个时刻（通常在少于1万个优化步骤内）**突然学会**了如何精准匹配输入的控制图像。作者将此现象称为“突然收敛”。



### 推理

#### 背景：无分类器引导 (Classifier-Free Guidance - CFG)

Stable Diffusion依赖CFG技术来提升图像质量和文本匹配度。其核心公式为：

<img src="https://latex.codecogs.com/svg.latex?\epsilon_{prd}&space;=&space;\epsilon_{uc}&space;&plus;&space;\beta_{cfg}(\epsilon_c&space;-&space;\epsilon_{uc})" alt="epsilon_prd = epsilon_uc + beta_cfg(epsilon_c - epsilon_uc)" />

其中，<img src="https://latex.codecogs.com/svg.latex?\epsilon_{uc}" alt="epsilon_uc" /> 是无条件（无文本提示）下的噪声预测，而 <img src="https://latex.codecogs.com/svg.latex?\epsilon_c" alt="epsilon_c" /> 是有条件下的预测。CFG通过一个权重 <img src="https://latex.codecogs.com/svg.latex?\beta_{cfg}" alt="beta_cfg" /> 来放大二者的差异，从而强化文本引导。

当引入ControlNet后，一个问题就出现了：ControlNet的控制信号应该加入到 <img src="https://latex.codecogs.com/svg.latex?\epsilon_{uc}" alt="epsilon_uc" /> 还是 <img src="https://latex.codecogs.com/svg.latex?\epsilon_c" alt="epsilon_c" />？研究发现，两种简单的做法都有缺陷。

#### 新方法：CFG分辨率加权 (CFG Resolution Weighting)

为了解决上述问题，论文提出了一种新的加权策略：

1.  首先，将ControlNet的图像条件**只添加到有条件的预测 <img src="https://latex.codecogs.com/svg.latex?\epsilon_c" alt="epsilon_c" /> 中**。
2.  然后，在ControlNet与Stable Diffusion的U-Net各层连接处，乘以一个权重 <img src="https://latex.codecogs.com/svg.latex?w_i" alt="w_i" />。这个权重与当前U-Net模块的分辨率 <img src="https://latex.codecogs.com/svg.latex?h_i" alt="h_i" /> 挂钩：

<img src="https://latex.codecogs.com/svg.latex?w_i&space;=&space;64/h_i" alt="w_i = 64/h_i" />

例如，对于分辨率为8x8的模块，权重为`64/8=8`；对于64x64的模块，权重为`64/64=1`。这种方法可以在调整CFG强度时获得更好的效果。

#### 多重ControlNet的组合 (Composing multiple ControlNets)

ControlNet的一个极其强大的功能是**可以同时使用多个控制条件**。例如，用户可以同时输入一张Canny边缘图和一张人物姿态图来控制一张图像的生成。

实现方式出奇的简单：**直接将多个对应ControlNet模型的输出相加即可**。

例如，将“Canny-ControlNet”的输出与“Pose-ControlNet”的输出在注入U-Net之前直接相加。这种简单的线性组合非常有效，不需要任何额外的加权或复杂的插值操作。